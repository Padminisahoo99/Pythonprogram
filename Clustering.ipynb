{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMuLAapXlI1oJRg2bEd7g+r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Perform clustering (hierarchical,K means clustering and DBSCAN) for the airlines data to obtain optimum number of clusters. Draw the inferences from the clusters obtained.\n","\n","Data Description:\n","\n","The file EastWestAirlinescontains information on passengers who belong to an airlineâ€™s frequent flier program. For each passenger the data include information on their mileage history and on different ways they accrued or spent miles in the last year. The goal is to try to identify clusters of passengers that have similar characteristics for the purpose of targeting different segments for different types of mileage offers\n","\n","ID --Unique ID\n","\n","Balance--Number of miles eligible for award travel\n","\n","Qual_mile--Number of miles counted as qualifying for Topflight status\n","\n","cc1_miles -- Number of miles earned with freq. flyer credit card in the past 12 months: cc2_miles -- Number of miles earned with Rewards credit card in the past 12 months: cc3_miles -- Number of miles earned with Small Business credit card in the past 12 months:\n","\n","1 = under 5,000 2 = 5,000 - 10,000 3 = 10,001 - 25,000 4 = 25,001 - 50,000 5 = over 50,000\n","\n","Bonus_miles--Number of miles earned from non-flight bonus transactions in the past 12 months\n","\n","Bonus_trans--Number of non-flight bonus transactions in the past 12 months\n","\n","Flight_miles_12mo--Number of flight miles in the past 12 months\n","\n","Flight_trans_12--Number of flight transactions in the past 12 months\n","\n","Days_since_enrolled--Number of days since enrolled in flier program\n","\n","Award--whether that person had award flight (free flight) or not"],"metadata":{"id":"Xn78i90oBiha"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5nsuj-4-5hD"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"pSsuWQ_Agq2_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Air = pd.read_csv(\"Copy of EastWestAirlines.csv\")\n","Air.head()"],"metadata":{"id":"MZN5cm2LgqzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## import hie. clustering lib\n","import scipy.cluster.hierarchy as sch ## for dendrogram and ploting\n","from sklearn.cluster import AgglomerativeClustering"],"metadata":{"id":"vh8qeDWYgqv9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## before calculatimg the distances we neeed to do standardization\n","## customized / user defined normalization function\n","## here we can standardized functions as well from  sklearn but to show you how we can write custom function\n","\n","def norm_func(i):\n","  x = (i-i.min())/(i.max()-i.min())\n","  return x                                            # (xi - min)/range (feature scaling ), range=max-min"],"metadata":{"id":"vY1LxZcWgqpL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## for our understanding\n","# (i-i.min())/(i-i.max()-i.max())\n","Air['Balance'].min() , Air['Balance'].max()"],"metadata":{"id":"TBetShuigql3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(0-0)/(1704838-0)"],"metadata":{"id":"A2s2_4R2gqik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Normalized Dataframe\n","Air.iloc[:,1:]"],"metadata":{"id":"LDXNc1FIgqfe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## consider all row and from 1: col to all columns to normalize the values\n","df_norm = norm_func(Air.iloc[:,1:])\n","df_norm"],"metadata":{"id":"zG2dk1wUgqcb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_norm.isna().sum()"],"metadata":{"id":"ixgYjd4ggqYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_norm.describe()"],"metadata":{"id":"Gt1lKzOFgqVL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.pairplot(df_norm)\n","plt.show()"],"metadata":{"id":"n4_avKLWgqR_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_norm.plot(kind='box', subplots=True, layout=(5,3), figsize=(25,10))\n","plt.show()"],"metadata":{"id":"hW_sUewvgqPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_norm.corr()"],"metadata":{"id":"-i5b7hA9gqLz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","fig, ax = plt.subplots(figsize=(15,15))\n","sns.heatmap(df_norm.corr(), annot=True, linewidths=0.2, linecolor=\"black\", fmt=\".2f\", ax=ax)\n","plt.show()"],"metadata":{"id":"ej3DvqNAgqJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## values get scale down then create dendrogram\n","\n","dendrogram = sch.dendrogram(sch.linkage(df_norm, method = 'single'))   ##  here using single linkage method\n","plt.figure(figsize=(70, 35))"],"metadata":{"id":"fSIsVhJFgqFv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["other methods"],"metadata":{"id":"EcNgxo_shNlj"}},{"cell_type":"code","source":["!pip install scipy"],"metadata":{"id":"gGRA1l12gqCr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.cluster.hierarchy import dendrogram, linkage\n","\n","import numpy as np"],"metadata":{"id":"25uH1KnRgp_V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## for the complete linkage method i have concerted data in numpy array\n","df_array = df_norm.to_numpy()"],"metadata":{"id":"VprWrn4thR_7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["linkage_matrix = linkage(df_array, method='complete')"],"metadata":{"id":"Fi1BfpS3hR8X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The dendrogram function is now called dendrogram1, so we use that instead.\n","dendrogram1 = dendrogram(linkage_matrix)"],"metadata":{"id":"7S67WePShR5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dendrogram2 = dendrogram(linkage(df_norm, method = 'centroid'))"],"metadata":{"id":"eKU7I9vWhR2S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create cluster, dont have mentioned that how many clusters we have to make so i am going with 3 clusters"],"metadata":{"id":"8Hi8sHqrhbZD"}},{"cell_type":"code","source":["hc = AgglomerativeClustering(n_clusters=3, affinity = 'euclidean', linkage= 'single')\n","hc"],"metadata":{"id":"qKjiGhUJhRzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## now i have created the clusters now i want to see the values are in which groups/clusters\n","## for that we are going to use fit predict method\n","y_hc = hc.fit_predict(df_norm)\n","clusters=pd.DataFrame(y_hc, columns=['clusters'])    ## append thses values in data frame"],"metadata":{"id":"RkRMHv0XhRwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_hc"],"metadata":{"id":"CCCU_ya2hRs2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# points to clusters.\n","clusters"],"metadata":{"id":"4WoWqJb1hkj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Air['cluster_id'] = clusters\n","Air"],"metadata":{"id":"cyVONnrIhkgu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Air_1 = Air.sort_values(\"cluster_id\")"],"metadata":{"id":"By5BBDIqhkdE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cluster_sort = Air_1.iloc[:, [0,12]]\n","cluster_sort"],"metadata":{"id":"Z1QdMwfDhkZ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cluster_sort['cluster_id'].sort_values()"],"metadata":{"id":"nXwIfaMihkWd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how to count unique values\n","## the number of customers to which clusters\n","Air['cluster_id'].value_counts()"],"metadata":{"id":"swZIQem4hkTQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["K-means clustering"],"metadata":{"id":"UM8zbDMJhyBk"}},{"cell_type":"code","source":["## import some lib.\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","## we dont know the no of clusters , so lets use k-means and elbow method to choose this number of optimal clusters\n","from sklearn.cluster import KMeans"],"metadata":{"id":"cWLoLlo4hkQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install scikit-learn"],"metadata":{"id":"37JKbm2zhkMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler    ## built in function for standardizati"],"metadata":{"id":"jVjLUqAohkJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Normalizatio / standardization function\n","\n","scaler = StandardScaler()  ## scaler is an object name of standardscaler class ## object intilizer"],"metadata":{"id":"ARKkJ-oUh5ci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tr_df = scaler.fit_transform(Air.iloc[:, 1:])\n","tr_df"],"metadata":{"id":"FnqYiEZIh5Y3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## how to find optimum number of culsters\n","## k-means algorithm aims to choose centroids that minimum the inertia or within cluster sum of squares intertia\n","import matplotlib.pyplot as plt"],"metadata":{"id":"wuNnuXQvh5Vn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## inertia meansures the how well a data set was calculated by k-means\n","# it s calculated by measuring the distance between each data point and its centriod, squaring this distance\n","# and suming these squares across one cluster.\n","## a good model is one with low inertia and low number of clusters(k)\n","\n","# to plot th eelbow method graph, we need to compute the WCSS (within cluster sum of squares)\n","# let say max no. of clusters could be 10\n","# as we are going to have 10 iterations we are going to write a for loop to create a list of 10 WCSS for the no of clusters.\n","wcss = []   ## whithin  the cluster sum of squares.intilize wcss and begin the loop\n","\n","for i in range(1, 11):\n","  kmeans = KMeans(n_clusters=i,init='k-means++', max_iter=300,random_state=0)  ## max ier 300 is default   ## inti = is random itilizer method\n","  kmeans.fit(tr_df)  ## use fit method to fit the kmeans object to our scaled data frame\n","  wcss.append(kmeans.inertia_) ## another name for wcss is inertia.in wcss list we will apend all distance ## this is like p and t value model.p-value\n","\n","plt.plot(range(1,11), wcss)\n","plt.title('elbow method')\n","plt.xlabel('no of clusters')\n","plt.ylabel('wcss')\n","plt.show()"],"metadata":{"id":"KLgMqupbh5SH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wcss"],"metadata":{"id":"A7PUTNulh5On"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kmeans = KMeans(n_clusters=4, random_state=0)  ## pass no of clusters as 4\n","predict = kmeans.fit_predict(tr_df)"],"metadata":{"id":"8VkVBeq8h5Kt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict"],"metadata":{"id":"BEoOT_Znh5Ds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clusters_new = KMeans(n_clusters=3, random_state=0)\n","new_predict=clusters_new.fit_predict(tr_df)"],"metadata":{"id":"jW0qrsvth5AL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clusters_new.labels_"],"metadata":{"id":"LwcxPUmHh488"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_predict"],"metadata":{"id":"QC_rnoo5h449"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Air['cluster_id'] = new_predict\n","Air.head()"],"metadata":{"id":"Da8MlVW4iN6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## group by the cluster id\n","Air.groupby('cluster_id').agg(['mean']).reset_index()"],"metadata":{"id":"9FaVxtUkiN3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Air[Air['cluster_id']==1]"],"metadata":{"id":"jHiJTm1ViN0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Air[Air['cluster_id']==2]"],"metadata":{"id":"pCaGQskhiNxL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(Air[Air['cluster_id']==0].shape[0])\n","print(Air[Air['cluster_id']==1].shape[1])"],"metadata":{"id":"ObCXwg2TiNt0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DB SCAN"],"metadata":{"id":"pTENu_E6iiNj"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"DnRVbswSiY-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Air"],"metadata":{"id":"z1h9lUWViY53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Air.info()"],"metadata":{"id":"SA63F__ciY2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Air.describe()"],"metadata":{"id":"XpSeB3EBiYyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Air.isna()"],"metadata":{"id":"UJmw2E0niYvB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Air.isna().sum()"],"metadata":{"id":"7PbUl3ccixXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## check for outliers\n","Air.boxplot()"],"metadata":{"id":"2ML4io13ixUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(50, 25))\n","Air.boxplot()\n","plt.show()\n","## huge amout of outliers are there"],"metadata":{"id":"w6-7YgVXixQj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sklearn"],"metadata":{"id":"pIQ0KtYNixLv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"SKGV3xIhixH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sc = StandardScaler()\n","Air = sc.fit_transform(Air)\n","Air"],"metadata":{"id":"OiwaBJHEixEo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## build model\n","from sklearn.cluster import DBSCAN"],"metadata":{"id":"tBaC4ygDjNyc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["db1 = DBSCAN()   ## db1 is instance  # default eps=0.5 , min_samples = 5\n","see = db1.fit_predict(Air)   ## we will get labels for clusters, -1 = outliers\n","see"],"metadata":{"id":"41lp5GKHjNt1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["db2 = DBSCAN(eps=1, min_samples=3)  ## we are trying some values\n","db2.fit_predict(Air)"],"metadata":{"id":"a8h6t7QhjNqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["db3 = DBSCAN(eps=1, min_samples=7)  ## we are trying some values\n","db3.fit_predict(Air)"],"metadata":{"id":"kDqznQ00jNnL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Df = pd.DataFrame(Air)"],"metadata":{"id":"wRVZUq5XjNjr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","sns.pairplot(df) ## pair plot is scatter plot in seaborn scatter plot is pairplo"],"metadata":{"id":"z9RDNaQljNgm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## evaluvate your model with silhoutte score\n","silhouette_score(Air,db1.fit_predict(Air)"],"metadata":{"id":"Ck4KGw58jNdD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["silhouette_score(Air,db2.fit_predict(Air) )"],"metadata":{"id":"dBOWiXx7jNYz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["silhouette_score(Air,db3.fit_predict(Air) )"],"metadata":{"id":"rEwlkSjAjNV1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["db4 = DBSCAN(eps=2, min_samples=7)  ## we are trying some values\n","db4.fit_predict(Air)\n","silhouette_score(Air,db4.fit_predict(Air) )"],"metadata":{"id":"zZX9HcYwjdx5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["db5 = DBSCAN(eps=3, min_samples=8)  ## we are trying some values\n","y = db5.fit_predict(Air)\n","silhouette_score(Air,db5.fit_predict(Air) )      ### this is the so far best value i get *********"],"metadata":{"id":"_h14WTTujduS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["db6 = DBSCAN(eps=4, min_samples=9)\n","db6.fit_predict(Air)"],"metadata":{"id":"argQ9kF2jdqU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["silhouette_score(Air,db6.fit_predict(Air) )"],"metadata":{"id":"hgrzeeDDjdm-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["db7 = DBSCAN(eps=3, min_samples=12)  ## we are trying some values\n","y = db7.fit_predict(Air)\n","silhouette_score(Air,db7.fit_predict(Air) )      ### this is the so far best value i get *********"],"metadata":{"id":"qh6q-7HxjdkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1 = pd.read_csv(\"Copy of EastWestAirlines.csv\")"],"metadata":{"id":"jDsGmpm1jdhP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1['cluster'] = y\n","df1"],"metadata":{"id":"Z4DlZhsFjoYi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##  extract outliers\n","df1[df1['cluster'] == -1]"],"metadata":{"id":"kCvLE7JAjoTT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NgiE6ZnfjoPF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mPhBPOtEjoJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uSCPTiQPjoGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VjhWCCBdjoDb"},"execution_count":null,"outputs":[]}]}